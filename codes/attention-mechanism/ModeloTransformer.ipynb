{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo um modelo transformer sem uso de framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um modelo Transformer consiste em várias partes principais:\n",
    "\n",
    "1. Camada de Embedding: Transforma as palavras em vetores numéricos de tamanho fixo\n",
    "2. Mecanismo de atenção: Permite que o modelo foque em diferentes partes da entrada\n",
    "3. Camada Encoder e Decoder: Processam os dados sequencialmente\n",
    "4. Camada Linear e Softmax: para predições finais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperparâmetros iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensao do modelo\n",
    "dim_model = 64\n",
    "\n",
    "# Comprimento da sequencia\n",
    "seq_length = 10\n",
    "\n",
    "# Tamanho do vocabulario\n",
    "vocab_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camada Embedding\n",
    "A função embedding é utilizada para converter entradas sequenciais em vetores densos de tamanho fixo. Esses vetores são conhecidos como embeddings e são uma parte fundamental, em especial dos modelos de PLN.\n",
    "\n",
    "Esse embeddings são fundamentais para modelos de aprendizado profundo em PLN, pois fornecem uma representação rica e densa de palavras ou tokens, capturando informações contextuais e semânticas que são essenciais para tarefas como tradução automática, classificação de texto, entre outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função para criar uma matriz embedding\n",
    "def embedding(input, vocab_size, dim_model):\n",
    "\n",
    "    # Cria uma matriz de embeddings onde cada linha representa um token do vacabulario\n",
    "    # A matriz é inicializada com valores aleatorios normalmente distribuidos\n",
    "    embed = np.random.randn(vocab_size, dim_model)\n",
    "\n",
    "    # Para cada indice de token no input, seleciona o embedding correspondente da matriz\n",
    "    # Retorna um array de embeddings correspondentes a sequencia de entrada\n",
    "    return np.array([embed[i] for i in input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecanismo de atenção\n",
    "\n",
    "No Transformer, Q, K e V são derivados da mesma entrada em camadas de atenção do encoder, mas de entradas diferentes no decoder (Q vem da saída da camada anterior do decoder, enquanto K e V vêm da saída do encoder). O mecanismo de atenção calcula um conjunto de pontuações (usando o produto escalar entre Q e K, daí o nome \"scaled dot-product attention\"), aplica uma função softmax para obter pesos de atenção e usa esses pesos para ponderar os values, criando uma saída que é uma combinação ponderada das informações relevantes de entrada.\n",
    "\n",
    "Este processo permite que o modelo dê \"atenção\" às partes mais relevantes da entrada para cada parte da saída, o que é especialmente útil em tarefas como tradução, onde a relevância de diferentes palavras da entrada pode variar dependendo da parte da frase que está sendo traduzida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Ativação e Softmax\n",
    "\n",
    "A função softmax é uma função de ativação amplamente utilizada em rede neurais, especialmente em cenários de classificação, onde é importante transformar valores brutos de saída (logits) em probabilidades que somam 1. Abaixo, está o código da função softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de ativação softmax\n",
    "def softmax(x):\n",
    "\n",
    "    # Calcula o exponencial de cada elemento do input, ajustado pelo máximo valor no input\n",
    "    # para evitar overflow numérico\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "\n",
    "    # Divide cada exponencial pelo somatório dos exponenciais ao longo do último eixo (axis = 1)\n",
    "    # O reshape (-1,1) garante que a divisão seja realizada corretamente em um contexto multidimensional\n",
    "    return e_x / e_x.sum(axis = 1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
