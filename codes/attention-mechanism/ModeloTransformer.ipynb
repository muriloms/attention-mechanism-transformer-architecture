{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo um modelo transformer sem uso de framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um modelo Transformer consiste em várias partes principais:\n",
    "\n",
    "1. Camada de Embedding: Transforma as palavras em vetores numéricos de tamanho fixo\n",
    "2. Mecanismo de atenção: Permite que o modelo foque em diferentes partes da entrada\n",
    "3. Camada Encoder e Decoder: Processam os dados sequencialmente\n",
    "4. Camada Linear e Softmax: para predições finais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperparâmetros iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensao do modelo\n",
    "dim_model = 64\n",
    "\n",
    "# Comprimento da sequencia\n",
    "seq_length = 10\n",
    "\n",
    "# Tamanho do vocabulario\n",
    "vocab_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camada Embedding\n",
    "A função embedding é utilizada para converter entradas sequenciais em vetores densos de tamanho fixo. Esses vetores são conhecidos como embeddings e são uma parte fundamental, em especial dos modelos de PLN.\n",
    "\n",
    "Esse embeddings são fundamentais para modelos de aprendizado profundo em PLN, pois fornecem uma representação rica e densa de palavras ou tokens, capturando informações contextuais e semânticas que são essenciais para tarefas como tradução automática, classificação de texto, entre outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função para criar uma matriz embedding\n",
    "def embedding(input, vocab_size, dim_model):\n",
    "\n",
    "    # Cria uma matriz de embeddings onde cada linha representa um token do vacabulario\n",
    "    # A matriz é inicializada com valores aleatorios normalmente distribuidos\n",
    "    embed = np.random.randn(vocab_size, dim_model)\n",
    "\n",
    "    # Para cada indice de token no input, seleciona o embedding correspondente da matriz\n",
    "    # Retorna um array de embeddings correspondentes a sequencia de entrada\n",
    "    return np.array([embed[i] for i in input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecanismo de atenção\n",
    "\n",
    "No Transformer, Q, K e V são derivados da mesma entrada em camadas de atenção do encoder, mas de entradas diferentes no decoder (Q vem da saída da camada anterior do decoder, enquanto K e V vêm da saída do encoder). O mecanismo de atenção calcula um conjunto de pontuações (usando o produto escalar entre Q e K, daí o nome \"scaled dot-product attention\"), aplica uma função softmax para obter pesos de atenção e usa esses pesos para ponderar os values, criando uma saída que é uma combinação ponderada das informações relevantes de entrada.\n",
    "\n",
    "Este processo permite que o modelo dê \"atenção\" às partes mais relevantes da entrada para cada parte da saída, o que é especialmente útil em tarefas como tradução, onde a relevância de diferentes palavras da entrada pode variar dependendo da parte da frase que está sendo traduzida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Ativação e Softmax\n",
    "\n",
    "A função softmax é uma função de ativação amplamente utilizada em rede neurais, especialmente em cenários de classificação, onde é importante transformar valores brutos de saída (logits) em probabilidades que somam 1. Abaixo, está o código da função softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de ativação softmax\n",
    "def softmax(x):\n",
    "\n",
    "    # Calcula o exponencial de cada elemento do input, ajustado pelo máximo valor no input\n",
    "    # para evitar overflow numérico\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "\n",
    "    # Divide cada exponencial pelo somatório dos exponenciais ao longo do último eixo (axis = 1)\n",
    "    # O reshape (-1,1) garante que a divisão seja realizada corretamente em um contexto multidimensional\n",
    "    return e_x / e_x.sum(axis = 1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Dot Product\n",
    "\n",
    "A função scaled_dot_product_attention() é um componente do mecanismo de atenção em modelos Transformer. Ela calcula a atenção entre conjuntos de queries (Q), keys (K) e values (V).\n",
    "\n",
    "Essencialmente, esse função permite que o modelo dê importância diferenciada a diferentes partes da entrada, um aspecto chave que torna os modelos Transformer particularmente eficazes para tarefas de PLN e outras tarefas sequenciais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função para calcular a atenção escalada por produto escalar\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "\n",
    "    # Calcula o produto escalar entre Q e a transposta de K\n",
    "    matmul_qk = np.dot(Q, K.T)\n",
    "\n",
    "    # Obtém a dimensão dos vetores de chave\n",
    "    depth = K.shape[-1]\n",
    "\n",
    "    # Escala os logits dividindo-os pela raiz quadrada da profundidade\n",
    "    logits = matmul_qk / np.sqrt(depth)\n",
    "\n",
    "    # Aplica a função softmax para obter os pesos da atenção\n",
    "    attention_weights = softmax(logits)\n",
    "\n",
    "    # Multiplica os pesos de atenção pelos valores de V para obter a saída final\n",
    "    output = np.dot(attention_weights, V)\n",
    "\n",
    "    # Retorna a saída ponderada\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saída do modelo com operação linear softmax\n",
    "\n",
    "A função linear_and_softmax() é uma combinação de uma camada linear seguida por uma função softmax, comumente usada em modelos de aprendizado profundo, especialmente em tarefas de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função que aplica uma transformação linear seguida de softmax\n",
    "def linear_and_softmax(input):\n",
    "\n",
    "    # Inicializa uma matriz de pesos com valores aleatórios normalmente distribuídos\n",
    "    # Esta matriz conecta cada dimensão do modelo (dim model) a cada palavra do vocabulário (vocab_size)\n",
    "    weights = np.random.randn(dim_model, vocab_size)\n",
    "\n",
    "    # Realiza a operação linear (produto escalar) entre a entrada e a matriz de pesos\n",
    "    # O resultado, logits, é um vetor que representa a entrada transformada em um espaço de maior dimensão\n",
    "    logits = np.dot(input, weights)\n",
    "\n",
    "    # Aplica a função softmax aos logits\n",
    "    # Isso transforma os logits em um vetor de probabilidades, onde cada elemento soma 1\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo o modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função do modelo final\n",
    "def transformer_model(input):\n",
    "\n",
    "    # Embedding\n",
    "    embedded_input = embedding(input, vocab_size, dim_model)\n",
    "\n",
    "    # Mecanismo de atenção\n",
    "    attention_output = scaled_dot_product_attention(embedded_input, embedded_input, embedded_input)\n",
    "\n",
    "    # Camada linear e softmax\n",
    "    output_probabilities = linear_and_softmax(attention_output)\n",
    "\n",
    "    # Escolhendo os índices com maior probabilidade\n",
    "    output_indices = np.argmax(output_probabilities, axis=1)\n",
    "\n",
    "    return output_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o modelo para previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dados aleatórios para a entrada do modelo\n",
    "input_sequence = np.random.randint(0, vocab_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequência de Entrada:  [53 61 22 12 76 54 71 90 14 40]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequência de Entrada: \", input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo previsões com o modelo\n",
    "output = transformer_model(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída do modelo:  [ 9 37 43 16 45 28 89 89 68 33]\n"
     ]
    }
   ],
   "source": [
    "print(\"Saída do modelo: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
